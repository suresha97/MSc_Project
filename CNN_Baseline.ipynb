{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suresha97/MSc_Project/blob/main/CNN_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpsnH-P5vPYx"
      },
      "source": [
        "# Setup Workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxhzUDNtK3aX"
      },
      "source": [
        "#load packages\n",
        "import scipy\n",
        "from scipy import signal\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import time\n",
        "from IPython import display\n",
        "import copy \n",
        "import pickle\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Normalize, Resize, ToTensor\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JErwRkETLN0l",
        "outputId": "a3fb6bc4-fe34-4445-f96b-f7886cafee09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuwFSqoS43K3"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbfpQ9CS7OH9"
      },
      "source": [
        "## Spectrograms\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJs3ku0qYinK",
        "outputId": "d9c34536-d455-481b-e42f-12db1e0a8e3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#List of strings for each particiapnt\n",
        "parts = []\n",
        "nums = [str(x) for x in range(1,10,1)]\n",
        "ten = ['0'+x for x in nums]\n",
        "others = [str(x) for x in range(10,24,1)]\n",
        "participants = ten+others\n",
        "\n",
        "ppg_full_list = [] # 32 elements of 40 x 80 x 80 arrays\n",
        "resp_full_list = []\n",
        "\n",
        "#Load spectorgrams of PPG and RESP signals for each particiapnt\n",
        "for part in participants: \n",
        "  l_ppg = np.load('/content/drive/My Drive/PHYSIO/Generated_Specs/PPG_28/ppg_spec_{}_28.npy'.format(part)) #40 x 80 x 80\n",
        "  l_resp = np.load('/content/drive/My Drive/PHYSIO/Generated_Specs/RESP_28/resp_spec_{}_28.npy'.format(part)) #40 x 80 x 80 \n",
        "\n",
        "  ppg_full_list.append(l_ppg)\n",
        "  resp_full_list.append(l_resp)\n",
        "\n",
        "print(len(ppg_full_list))\n",
        "ppg_full_array = np.vstack(ppg_full_list)\n",
        "print(ppg_full_array.shape)\n",
        "\n",
        "print(len(resp_full_list))\n",
        "resp_full_array = np.vstack(resp_full_list)\n",
        "print(resp_full_array.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23\n",
            "(920, 28, 28)\n",
            "23\n",
            "(920, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7aOUFK17U7Z"
      },
      "source": [
        "## Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C6_FTL8D2AO"
      },
      "source": [
        "#Load either K-means clusters data, manual threshold clusters or normalised threshold clusters\n",
        "#labels_df = pd.read_csv(\"/content/drive/My Drive/PHYSIO/Generated_Specs/all_labels.csv\") #class_4 only in all_labels.csv\n",
        "labels_tdf = pd.read_csv(\"/content/drive/My Drive/PHYSIO/Generated_Specs/all_labels_threshold.csv\") \n",
        "#labels_df = pd.read_csv(\"/content/drive/My Drive/PHYSIO/Generated_Specs/threshold_norm.csv\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpgsiykrJF89",
        "outputId": "68707f16-baa1-49bc-930e-e4a92ee3160b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "labels_tdf.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>val</th>\n",
              "      <th>aro</th>\n",
              "      <th>dom</th>\n",
              "      <th>lik</th>\n",
              "      <th>val3</th>\n",
              "      <th>aro3</th>\n",
              "      <th>dom3</th>\n",
              "      <th>lik3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   val  aro  dom  lik  val3  aro3  dom3  lik3\n",
              "0    1    1    1    1     2     2     2     2\n",
              "1    1    1    1    1     2     2     2     2\n",
              "2    1    1    1    1     2     2     2     2\n",
              "3    1    1    1    1     1     2     2     2\n",
              "4    1    0    1    1     2     0     2     2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XVfzIHX6LGx",
        "outputId": "61b9e31f-5864-4d4f-d6aa-0af2f7175a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "#Print class proportion numbers for current clustering method\n",
        "for array in list(labels_tdf.columns):\n",
        "  (unique, counts) = np.unique(labels_tdf[array][0:920].to_numpy(), return_counts=True)\n",
        "  frequencies = np.asarray((unique, counts/920)).T\n",
        "  print(str(array))\n",
        "  print(frequencies)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val\n",
            "[[0.         0.37173913]\n",
            " [1.         0.62826087]]\n",
            "\n",
            "aro\n",
            "[[0.         0.36956522]\n",
            " [1.         0.63043478]]\n",
            "\n",
            "dom\n",
            "[[0.         0.37173913]\n",
            " [1.         0.62826087]]\n",
            "\n",
            "lik\n",
            "[[0.         0.29130435]\n",
            " [1.         0.70869565]]\n",
            "\n",
            "val3\n",
            "[[0.         0.29021739]\n",
            " [1.         0.3       ]\n",
            " [2.         0.40978261]]\n",
            "\n",
            "aro3\n",
            "[[0.         0.29130435]\n",
            " [1.         0.31413043]\n",
            " [2.         0.39456522]]\n",
            "\n",
            "dom3\n",
            "[[0.         0.30217391]\n",
            " [1.         0.33913043]\n",
            " [2.         0.35869565]]\n",
            "\n",
            "lik3\n",
            "[[0.         0.24673913]\n",
            " [1.         0.21086957]\n",
            " [2.         0.5423913 ]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsyVeIBn714s"
      },
      "source": [
        "## Run to Combine K-means Clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0vAmcvHuztd",
        "outputId": "2cee2df1-03b4-4a3c-a471-ccd793003b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "#Combine either the 1st and 2nd cluster into 1 or the 2nd and 3rd cluster into 1\n",
        "labels_newk = {}\n",
        "val_temp = labels_df['val3'].to_numpy()\n",
        "where_1 = np.where(val_temp == 1)\n",
        "val_temp[where_1] = 0\n",
        "where_2 = np.where(val_temp == 2)\n",
        "val_temp[where_2] = 1\n",
        "\n",
        "aro_temp = labels_df['aro3'].to_numpy()\n",
        "where_1 = np.where(aro_temp == 1)\n",
        "aro_temp[where_1] = 0\n",
        "where_2 = np.where(aro_temp == 2)\n",
        "aro_temp[where_2] = 1\n",
        "\n",
        "lik_temp = labels_df['lik3'].to_numpy()\n",
        "where_1 = np.where(lik_temp == 1)\n",
        "lik_temp[where_1] = 0\n",
        "where_2 = np.where(lik_temp == 2)\n",
        "lik_temp[where_2] = 1\n",
        "\n",
        "labels_newk['val'] = val_temp\n",
        "labels_newk['aro'] = aro_temp\n",
        "labels_newk['lik'] = lik_temp\n",
        "\n",
        "#Revised class proprtions after combinging clusters\n",
        "lab_newk = pd.DataFrame.from_dict(labels_newk)\n",
        "for array in list(lab_newk.columns):\n",
        "  (unique, counts) = np.unique(lab_newk[array][0:920].to_numpy(), return_counts=True)\n",
        "  frequencies = np.asarray((unique, counts)).T\n",
        "  print(str(array))\n",
        "  print(frequencies)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val\n",
            "[[  0 563]\n",
            " [  1 357]]\n",
            "\n",
            "aro\n",
            "[[  0 641]\n",
            " [  1 279]]\n",
            "\n",
            "lik\n",
            "[[  0 536]\n",
            " [  1 384]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfZH1_FZIbdI"
      },
      "source": [
        "# Prepare Data for Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zQDkufdKZ1t"
      },
      "source": [
        "#Name signal arrays and choose clustering type to train\n",
        "data_X = ppg_full_array\n",
        "data_X2 = resp_full_array\n",
        "data_Y = labels_tdf['aro'][0:len(data_X)].to_numpy()\n",
        "\n",
        "#data_Y = lab_newk['val'][0:len(data_X)].to_numpy()\n",
        "#Label encoding for clusterins with quadrant names as labels\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#data_Y = le.fit_transform(data_Y)\n",
        "#print(data_Y[0:20])\n",
        "\n",
        "image_size = data_X.shape[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTBW62hkkitj"
      },
      "source": [
        "## Run For Standardisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogYa4_SUaFz5"
      },
      "source": [
        "#Convert arrays to tensor\n",
        "all_x = torch.tensor(data_X)\n",
        "all_x2 = torch.tensor(data_X2)\n",
        "\n",
        "#Find mean and SD for spectrograms of each signal\n",
        "mean1 = all_x.mean().item()\n",
        "std1 = all_x.std().item()\n",
        "mean2 = all_x2.mean().item()\n",
        "std2 = all_x2.std().item()\n",
        "\n",
        "#Normalise data to have 0 mean and standard deviation of 1 i.e. be between [-1,1]\n",
        "transform = transforms.Compose([transforms.Normalize(mean1,std1)])\n",
        "transform2 = transforms.Compose([transforms.Normalize(mean2,std2)])\n",
        "\n",
        "all_x = transform(all_x)\n",
        "all_x = all_x.numpy()\n",
        "\n",
        "all_x2 = transform2(all_x2)\n",
        "all_x2 = all_x2.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MxbdM8sklLF"
      },
      "source": [
        "## Run for MinMax Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re5ootCQknj1",
        "outputId": "f8c291bf-8f51-4461-9c12-98b0965dda85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#Find min and max values for spectrograms of each signal\n",
        "ppg_max = np.amax(data_X)\n",
        "ppg_min = np.amin(data_X)\n",
        "resp_max = np.amax(data_X2)\n",
        "resp_min = np.amin(data_X2)\n",
        "\n",
        "print(ppg_max)\n",
        "print(ppg_min)\n",
        "print(resp_max)\n",
        "print(resp_min)\n",
        "\n",
        "#Apply min max normalisation to data so that all values are between 0 and 1\n",
        "data_X = (data_X - ppg_min)/(ppg_max-ppg_min)\n",
        "data_X2 = (data_X2 - resp_min)/(resp_max-resp_min)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.76465565\n",
            "8.650511e-06\n",
            "0.8332644\n",
            "8.939816e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJiAxFcc9KSF",
        "outputId": "20ad1faf-8364-4603-d03a-17a51740cda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#For 2 channel input, convert seperate siganl arrays to 920 x 2 x 28 x 28\n",
        "to_stack = [data_X, data_X2]\n",
        "two_sig = np.stack(np.asarray(to_stack), axis = 1) \n",
        "print(two_sig.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(920, 2, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EJsvEWbtgaB"
      },
      "source": [
        "## Data Splits - Single Signal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxMbeXz4I-Af",
        "outputId": "6b169d87-f41e-42a5-d924-5cbdcedccc84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "def data_split(data_feat,data_lab, channel):\n",
        "  #Make training + (val+test) sets 60/40 \n",
        "  X_train, X_test, y_train, y_test = train_test_split(data_feat, data_lab, \n",
        "                                                      train_size=0.6, \n",
        "                                                      random_state=42,\n",
        "                                                      stratify=data_lab)\n",
        "\n",
        "  print(f\"Numbers of train instances by class: {np.bincount(y_train)}\")\n",
        "\n",
        "  #Make val and test sets 50/50 \n",
        "  X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n",
        "                                                      train_size=0.5, \n",
        "                                                      random_state=42,\n",
        "                                                      stratify=y_test)\n",
        "\n",
        "\n",
        "  print(f\"Numbers of test instances by class: {np.bincount(y_test)}\")\n",
        "  print(f\"Numbers of validation instances by class: {np.bincount(y_val)}\")\n",
        "\n",
        "  #If single channel reshape to satisfy convolional layer operations\n",
        "  if channel == 1:\n",
        "    X_train = X_train.reshape(len(X_train),1,image_size,image_size)\n",
        "    X_val = X_val.reshape(len(X_val),1,image_size,image_size)\n",
        "    X_test = X_test.reshape(len(X_test),1,image_size,image_size)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "  \n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = data_split(data_X,data_Y, channel = 1)\n",
        "\n",
        "#Convert to torch tensors\n",
        "trainf_tensor = torch.tensor(X_train, dtype=torch.float)\n",
        "trainl_tensor = torch.tensor(y_train, dtype=torch.float)\n",
        "valf_tensor = torch.tensor(X_val, requires_grad=False, dtype=torch.float)\n",
        "vall_tensor = torch.tensor(y_val, requires_grad=False, dtype=torch.float)\n",
        "testf_tensor = torch.tensor(X_test, requires_grad=False, dtype=torch.float)\n",
        "testl_tensor = torch.tensor(y_test, requires_grad=False, dtype=torch.float)\n",
        "\n",
        "#Batchify training data with trainloader\n",
        "batch_size = 50\n",
        "trainset = torch.utils.data.TensorDataset(trainf_tensor, trainl_tensor)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numbers of train instances by class: [204 348]\n",
            "Numbers of test instances by class: [ 68 116]\n",
            "Numbers of validation instances by class: [ 68 116]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oldx3PslBf8O",
        "outputId": "774b253b-ed45-4159-e345-ab8bf45724e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(552, 1, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8OLcVgHtkXs"
      },
      "source": [
        "## Data Splits - Multi Signal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClWosXOt7wBc",
        "outputId": "02ead90b-3c4f-431f-8c05-89b02065135b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "\n",
        "#split based on indices - didn't stratify classes. \n",
        "def multi_data_split(data_feats1,data_feats2,data_labs):\n",
        "\n",
        "  #Train test indices\n",
        "  sss = StratifiedShuffleSplit(n_splits=1,test_size=0.4,train_size=0.6,random_state=42)\n",
        "  sss.get_n_splits(data_feats1, data_labs)\n",
        "  train_index, test_index = next(sss.split(data_feats1, data_labs))\n",
        "  print(len(train_index))\n",
        "  print(len(test_index))\n",
        "\n",
        "  #Further divide test indices to val and test indices\n",
        "  data_feat1_split = np.take(data_feats1, test_index, axis =0)\n",
        "  data_labs_split = np.take(data_labs, test_index, axis =0)\n",
        "  sss = StratifiedShuffleSplit(n_splits=1,test_size=0.5,train_size=0.5,random_state=42)\n",
        "  sss.get_n_splits(data_feat1_split, data_labs_split)\n",
        "  test_index, val_index = next(sss.split(data_feat1_split, data_labs_split))\n",
        "  print(len(test_index))\n",
        "  print(len(val_index))\n",
        "\n",
        "  #Training set\n",
        "  X_train1 = np.take(data_feats1, train_index, axis = 0)\n",
        "  X_train2 = np.take(data_feats2, train_index, axis = 0)\n",
        "  y_train = np.take(data_labs, train_index)\n",
        "\n",
        "  #Validation set\n",
        "  X_val1 = np.take(data_feats1, val_index, axis = 0)\n",
        "  X_val2 = np.take(data_feats2, val_index, axis = 0)\n",
        "  y_val = np.take(data_labs, val_index)\n",
        "\n",
        "  #Test set\n",
        "  X_test1 = np.take(data_feats1, test_index, axis = 0)\n",
        "  X_test2 = np.take(data_feats2, test_index, axis = 0)\n",
        "  y_test = np.take(data_labs, test_index)\n",
        "\n",
        "  #Reshape all of the above into to specifcy number of input channels\n",
        "  X_train1 = X_train1.reshape(len(X_train1),1,image_size, image_size)\n",
        "  X_val1 = X_val1.reshape(len(X_val1),1,image_size,image_size)\n",
        "  X_test1 = X_test1.reshape(len(X_test1),1,image_size,image_size)\n",
        "\n",
        "  X_train2 = X_train2.reshape(len(X_train2),1,image_size,image_size)\n",
        "  X_val2 = X_val2.reshape(len(X_val2),1,image_size,image_size)\n",
        "  X_test2 = X_test2.reshape(len(X_test2),1,image_size,image_size)\n",
        "  return X_train1, X_train2, X_val1, X_val2, X_test1, X_test2, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "X_train_ppg, X_train_resp, X_val_ppg, X_val_resp, X_test_ppg, X_test_resp, y_train, y_val, y_test = multi_data_split(data_X,data_X2,data_Y)\n",
        "\n",
        "\n",
        "#Convert to torch tensors\n",
        "trainf_tensor1 = torch.tensor(X_train_ppg, dtype=torch.float)\n",
        "valf_tensor1 = torch.tensor(X_val_ppg, requires_grad=False, dtype=torch.float)\n",
        "testf_tensor1 = torch.tensor(X_test_ppg, requires_grad=False, dtype=torch.float)\n",
        "\n",
        "trainf_tensor2 = torch.tensor(X_train_resp, dtype=torch.float)\n",
        "valf_tensor2 = torch.tensor(X_val_resp, requires_grad=False, dtype=torch.float)\n",
        "testf_tensor2 = torch.tensor(X_test_resp, requires_grad=False, dtype=torch.float)\n",
        "\n",
        "trainl_tensor = torch.tensor(y_train, dtype=torch.float)\n",
        "vall_tensor = torch.tensor(y_val, requires_grad=False, dtype=torch.float)\n",
        "testl_tensor = torch.tensor(y_test, requires_grad=False, dtype=torch.float)\n",
        "\n",
        "#Batchify training data uisng trainloader\n",
        "batch_size = 32\n",
        "trainset = torch.utils.data.TensorDataset(trainf_tensor1, trainf_tensor2, trainl_tensor)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "552\n",
            "368\n",
            "184\n",
            "184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLShenCMKS9T",
        "outputId": "0b25f707-1fb4-401c-eeca-1292521b2eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Number of batches with the current batch size\n",
        "print(len(trainloader))\n",
        "batchsize = len(trainloader)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLMcULyCKdV4"
      },
      "source": [
        "# Define Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-y-KcwRA43L"
      },
      "source": [
        "\n",
        "def accuracy_2(t_batch):\n",
        "  # Compute the accuracy of the current batch\n",
        "  correct_pred = 0\n",
        "  total_pred = 0\n",
        "  for data in t_batch:\n",
        "      images, labels = t_batch\n",
        "      # Compute the predicted labels\n",
        "      outputs = net.forward(Variable(images))\n",
        "      dummy, pred_labels = torch.max(outputs.data, 1)\n",
        "      \n",
        "      # Count the correct predictions\n",
        "      correct_pred += (pred_labels == labels).sum().item()\n",
        "      total_pred += pred_labels.size(0)\n",
        "      \n",
        "  # Add accuracy to the overall accuracy\n",
        "  acc = (100 * correct_pred)/total_pred\n",
        "\n",
        "  return acc\n",
        "\n",
        "#Accuracy function\n",
        "def accuracy(features1,features2, labels,signal):\n",
        "  #Get predictions from model\n",
        "  if signal == 'Single':\n",
        "    outputs = net.forward(features1)\n",
        "    #outputs = F.softmax(outputs, dim = 0)\n",
        "  if signal == 'Multi':\n",
        "    outputs = net.forward(features1, features2)\n",
        "  #_, preds = torch.max(outputs.data,1)\n",
        "\n",
        "  preds = []\n",
        "  for el in outputs.data:\n",
        "    if el >= 0.5:\n",
        "      preds.append(1)\n",
        "    elif el < 0.5:\n",
        "      preds.append(0)\n",
        "\n",
        "  preds = np.array(preds)\n",
        "\n",
        "  #Find number that are coorect and accuracy\n",
        "  num_correct = (torch.tensor(preds) == labels.int()).sum().numpy()\n",
        "  accuracy = num_correct/(labels.size()[0])\n",
        "\n",
        "  return accuracy*100\n",
        "\n",
        "\n",
        "def F1(features,feats2,true_y,signal):\n",
        "  #Get predictions from model\n",
        "  if signal == 'Single':\n",
        "    outputs = net.forward(features)\n",
        "    outputs = F.softmax(outputs, dim = 0)\n",
        "  if signal == 'Multi':\n",
        "    outputs = net.forward(features, feats2)\n",
        "  _,preds = torch.max(outputs.data,1)\n",
        "  #F1 using sklearn metrics\n",
        "  true_y = true_y.detach().numpy()\n",
        "  F1 = sklearn.metrics.f1_score(true_y, preds)\n",
        "\n",
        "  return F1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7NjS43Epo10"
      },
      "source": [
        "# Model Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpV3Cp2XKhPK"
      },
      "source": [
        "## Single Signal (Baseline CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb-Axgw9xwNe"
      },
      "source": [
        "\n",
        "np.random.seed(3)\n",
        "\n",
        "# Module is a base class for all neural network modules\n",
        "class Single_Net(nn.Module):\n",
        "    \n",
        "    # Define the neural network layers\n",
        "    def __init__(self,fcsize):\n",
        "        \n",
        "        super(Single_Net, self).__init__()\n",
        "        self.fcsize = fcsize\n",
        "\n",
        "        #Conv layers\n",
        "        self.conv1 = nn.Conv2d(1, 9, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(9, 18, kernel_size=5)\n",
        "\n",
        "        #FC layers\n",
        "        self.fc1 = nn.Linear(self.fcsize, 50)\n",
        "        self.fc2 = nn.Linear(50,1)\n",
        "\n",
        "    # Define the forward pass.    \n",
        "    def forward(self, x):\n",
        "        #Conv1 Block 1 - Convolution + Pooling + Non-linearity\n",
        "        x = F.max_pool2d(self.conv1(x), 2, stride = 2)\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        #Conv1 Block 2 - Convolution + Pooling + Non-linearity\n",
        "        x = F.max_pool2d(self.conv2(x), 2, stride = 2)\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        #Flatten array\n",
        "        x = x.view(-1, self.fcsize)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGTEfDcrxzjI"
      },
      "source": [
        "## Signal Fusion (Baseline CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOTrPcdMx2NR"
      },
      "source": [
        "#np.random.seed(3)\n",
        "\n",
        "# Module is a base class for all neural network modules\n",
        "class Multi_Net(nn.Module):\n",
        "    \n",
        "    # Define the neural network layers\n",
        "    def __init__(self, fcsize):\n",
        "        \n",
        "        super(Multi_Net, self).__init__()\n",
        "        self.fcsize = fcsize\n",
        "        #Conv layers\n",
        "        self.conv1 = nn.Conv2d(1, 5, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5)\n",
        "\n",
        "        #FC layers\n",
        "        self.fc1 = nn.Linear(self.fcsize*2, self.fcsize)\n",
        "        self.fc2 = nn.Linear(self.fcsize, int(fcsize/2))\n",
        "        self.fc3 = nn.Linear(int(fcsize/2),2)\n",
        "\n",
        "    # Define the forward pass.    \n",
        "    def forward(self, x, x2):\n",
        "        #Conv1 Block 1 (Singal 1) - Convolution + Pooling + Non-linearity\n",
        "        x = F.max_pool2d(self.conv1(x), 2)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        #Conv1 Block 2 (Singal 1) - Convolution + Pooling + Non-linearity\n",
        "        x = F.max_pool2d(self.conv2(x), 2)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        #Conv1 Block 1 (Singal 2) - Convolution + Pooling + Non-linearity\n",
        "        x2 = F.max_pool2d(self.conv1(x2), 2)\n",
        "        x2 = F.relu(x2)\n",
        "        \n",
        "        #Conv1 Block 2 (Singal 2) - Convolution + Pooling + Non-linearity\n",
        "        x2 = F.max_pool2d(self.conv2(x2), 2)\n",
        "        x2 = F.relu(x2)\n",
        "        \n",
        "        #Flatten layers for both signal\n",
        "        x = x.view(-1, self.fcsize)\n",
        "        x2 = x2.view(-1, self.fcsize)\n",
        "        #Concatenate features extracted from both spectrograms\n",
        "        x_combined = torch.cat((x,x2),dim=1)\n",
        "\n",
        "        #FC layers, non-linearities and predcition layers\n",
        "        x_combined = F.relu(self.fc1(x_combined))\n",
        "        x_combined = F.relu(self.fc2(x_combined))\n",
        "        x_combined = self.fc3(x_combined) \n",
        "\n",
        "        return x_combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvUp2fUiZa20"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tz5um1sIcPX"
      },
      "source": [
        "\n",
        "# Train the CNN\n",
        "def train_net(train_set, no_epochs, lr, m, opt, classes, signal='Single'):\n",
        "\n",
        "    #Define the loss and optimiser\n",
        "    \n",
        "    #loss_func = nn.CrossEntropyLoss()\n",
        "    loss_func = nn.BCELoss()\n",
        "\n",
        "    if opt == 'ADAM':\n",
        "      optimizer = optim.Adam(net.parameters(), lr = lr)\n",
        "    elif opt == 'SGD':\n",
        "      optimizer=optim.SGD(net.parameters(), lr = lr, momentum = m)\n",
        "\n",
        "    #Initalise best loss and accuracies and lists to store values\n",
        "    best_val_loss = 10000\n",
        "    best_epoch = 0\n",
        "    best_val_acc = 0\n",
        "\n",
        "    losses_train = []\n",
        "    losses_val = []\n",
        "\n",
        "    # Loop over the number of epochs\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        #Initialise loss and acc\n",
        "        current_loss = 0.0\n",
        "        current_accuracy = 0.0\n",
        "\n",
        "        # Loop over each mini-batch\n",
        "        for batch_index, training_batch in enumerate(train_set, 0):\n",
        "  \n",
        "            #Load the mini-batch and wrap with variable\n",
        "            if signal == 'Single':\n",
        "              inputs, labels = training_batch\n",
        "              inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "            if signal == 'Multi':\n",
        "              inputs, inputs2, labels = training_batch\n",
        "              inputs, inputs2, labels = Variable(inputs), Variable(inputs2), Variable(labels)\n",
        "\n",
        "            #Initalise parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #Forward pass\n",
        "            if signal == 'Single':\n",
        "              outputs = net.forward(inputs)\n",
        "            \n",
        "            if signal == 'Multi':\n",
        "              outputs = net.forward(inputs, inputs2)\n",
        "\n",
        "            #labels = labels.long()\n",
        "            loss = loss_func(outputs, labels)\n",
        "            \n",
        "            #Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #Add loss \n",
        "            current_loss += loss.item()\n",
        "\n",
        "            # Add accuracy to the overall accuracy\n",
        "            #current_accuracy += accuracy_2(training_batch) \n",
        "            if signal == 'Single':\n",
        "              current_accuracy += accuracy(inputs,'_',labels, signal=signal)\n",
        "            if signal == 'Multi':\n",
        "              current_accuracy += accuracy(inputs,inputs2,labels,signal=signal)\n",
        "        '''\n",
        "        #Validation loss\n",
        "        if signal == 'Single':\n",
        "          val_preds = net.forward(valf_tensor)\n",
        "        if signal == 'Multi':\n",
        "          val_preds = net.forward(valf_tensor1, valf_tensor2)\n",
        "\n",
        "        current_val_loss = loss_func(val_preds,vall_tensor).item()\n",
        "\n",
        "        #Update best model\n",
        "        if current_val_loss < best_val_loss:\n",
        "          best_val_loss = current_val_loss\n",
        "          best_epoch = epoch\n",
        "          best_model = copy.deepcopy(net)\n",
        "\n",
        "        #current_val_acc = accuracy(valf_tensor, vall_tensor)\n",
        "        #if current_val_acc > best_val_acc:\n",
        "        #  best_val_acc = current_val_acc\n",
        "        #  best_epoch = epoch\n",
        "        #  best_model = copy.deepcopy(net)\n",
        "        '''\n",
        "        print('[Epoch: %d Batch: %5d] loss: %.3f, acc: %.3f' % \n",
        "                    (epoch + 1, batch_index+1, current_loss/batchsize, current_accuracy/batchsize))\n",
        "        losses_train.append(current_loss/batchsize)\n",
        "        #losses_val.append(current_val_loss) \n",
        "\n",
        "        #Early stopping criteria\n",
        "        #if epoch - best_epoch > 10:\n",
        "\n",
        "    print(\"------------------------------------------------------\")\n",
        "    print('Training has finished')\n",
        "    print('Best Epoch :',best_epoch,'Best Validation Loss :', best_val_loss)\n",
        "\n",
        "    #Find test set predictions\n",
        "    if signal == 'Single':\n",
        "      outputs = net.forward(testf_tensor)\n",
        "      #outputs = F.softmax(outputs, dim = 0)\n",
        "    if signal == 'Multi':\n",
        "      outputs = best_model.forward(testf_tensor1, testf_tensor2)\n",
        "\n",
        "    #_, preds = torch.max(outputs.data,1)\n",
        "    preds = []\n",
        "    for el in outputs.data:\n",
        "      if el >= 0.5:\n",
        "        preds.append(1)\n",
        "      elif el < 0.5:\n",
        "        preds.append(0)\n",
        "\n",
        "    #Test accuracy\n",
        "    num_correct = (torch.tensor(np.array(preds)).int() == testl_tensor.int()).sum().numpy()\n",
        "    test_ac = num_correct/(testl_tensor.size()[0])\n",
        "    print('Test Accuracy: ',test_ac*100)\n",
        "\n",
        "    #Test F1 score\n",
        "    true_y = testl_tensor.detach().numpy()\n",
        "    F1_test = sklearn.metrics.f1_score(true_y, preds, average = 'weighted')\n",
        "    print('Test F1 Score :', F1_test)\n",
        "    print(\"All :\",sklearn.metrics.classification_report(true_y, preds))\n",
        "    print(\"Confusion Matrix :\")\n",
        "    print(sklearn.metrics.confusion_matrix(true_y, preds))\n",
        "    #sklearn.metrics.plot_confusion_matrix(best_model, testf_tensor)\n",
        "\n",
        "    #Plot learning curves\n",
        "    fig = plt.figure(figsize=plt.figaspect(0.2))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.plot(losses_train,'r')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    #ax1 = fig.add_subplot(1, 2, 2)\n",
        "    #ax1.plot(losses_val)\n",
        "    #plt.xlabel('Epoch')\n",
        "    #plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "        \n",
        "\n",
        "    return test_ac*100, F1_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyQklCa4p5hN"
      },
      "source": [
        "# Train Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBvuaTAsspmU"
      },
      "source": [
        "## Train Single Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ8JgXGdwlO-"
      },
      "source": [
        "# Create a neural network\n",
        "# Set a number of epochs\n",
        "no_epochs = 300\n",
        "\n",
        "# Set the learning rate\n",
        "lr = 0.5\n",
        "\n",
        "# Set the momentum\n",
        "momentum = 0.9\n",
        "\n",
        "#490 for 28, pad = 2\n",
        "accuracies_test = []\n",
        "f1s = []\n",
        "\n",
        "#Train network 10 times and find average metrics\n",
        "for i in range(1):\n",
        "  print(\"Iteration : \",i+1)\n",
        "  net = Single_Net(288)\n",
        "  test_accuracy,testf1 = train_net(trainloader, no_epochs, lr, momentum, opt = 'SGD', classes = 'binary', signal = 'Single')   \n",
        "  accuracies_test.append(test_accuracy) \n",
        "  f1s.append(testf1)\n",
        "\n",
        "avg_acc = sum(accuracies_test)/len(accuracies_test)\n",
        "print(\"Accuracies :\", accuracies_test)\n",
        "print(\"Average test accuracy after 10 runs : {}%\".format(round(avg_acc,2)))\n",
        "print(\"Average test F1 Score :\", sum(f1s)/len(f1s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA1EADfFsr69"
      },
      "source": [
        "## Train Multi Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUyxYzQUwn-8"
      },
      "source": [
        "# Both spectrgroams through seperate \n",
        "# Set a number of epochs\n",
        "no_epochs = 500\n",
        "\n",
        "# Set the learning rate\n",
        "lr = 0.001\n",
        "\n",
        "# Set the momentum\n",
        "momentum = 0.9\n",
        "\n",
        "accuracies_test = []\n",
        "f1s = []\n",
        "\n",
        "#Train network 10 times and find average metrics\n",
        "for i in range(10):\n",
        "  print(\"Iteration : \",i+1)\n",
        "  net = Multi_Net(160)\n",
        "  test_accuracy,testf1 = train_net(trainloader, no_epochs, lr, momentum, opt = 'SGD', classes = 'binary', signal = 'Multi')   \n",
        "  accuracies_test.append(test_accuracy) \n",
        "  f1s.append(testf1)\n",
        "\n",
        "avg_acc = sum(accuracies_test)/len(accuracies_test)\n",
        "print(\"Accuracies :\", accuracies_test)\n",
        "print(\"Average test accuracy after 10 runs : {}%\".format(round(avg_acc,2)))\n",
        "print(\"Average test F1 Score :\", sum(f1s)/len(f1s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugQxYv3T00TQ"
      },
      "source": [
        "# LOSO Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQAKRIAn78qA",
        "outputId": "f09cd362-1f1b-4319-f1a1-dc6d11811a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#List of strings for each particiapnt\n",
        "parts = []\n",
        "nums = [str(x) for x in range(1,10,1)]\n",
        "ten = ['0'+x for x in nums]\n",
        "others = [str(x) for x in range(10,24,1)]\n",
        "participants = ten+others\n",
        "\n",
        "ppg_full_list = [] # 32 elements of 40 x 80 x 80 arrays\n",
        "resp_full_list = []\n",
        "\n",
        "#Load spectorgrams of PPG and RESP signals for each particiapnt\n",
        "for part in participants: \n",
        "  l_ppg = np.load('/content/drive/My Drive/PHYSIO/Generated_Specs/PPG_28/ppg_spec_{}_28.npy'.format(part)) #40 x 80 x 80\n",
        "  l_resp = np.load('/content/drive/My Drive/PHYSIO/Generated_Specs/RESP_28/resp_spec_{}_28.npy'.format(part)) #40 x 80 x 80 \n",
        "\n",
        "  ppg_full_list.append(l_ppg)\n",
        "  resp_full_list.append(l_resp)\n",
        "\n",
        "print(len(ppg_full_list))\n",
        "ppg_full_array = np.vstack(ppg_full_list)\n",
        "print(ppg_full_array.shape)\n",
        "\n",
        "print(len(resp_full_list))\n",
        "resp_full_array = np.vstack(resp_full_list)\n",
        "print(resp_full_array.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23\n",
            "(920, 28, 28)\n",
            "23\n",
            "(920, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sM_SPP9dTTk",
        "outputId": "071f78a2-07c7-4a05-c36f-c1808604ace1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "labels_tdf.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>val</th>\n",
              "      <th>aro</th>\n",
              "      <th>dom</th>\n",
              "      <th>lik</th>\n",
              "      <th>val3</th>\n",
              "      <th>aro3</th>\n",
              "      <th>dom3</th>\n",
              "      <th>lik3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   val  aro  dom  lik  val3  aro3  dom3  lik3\n",
              "0    1    1    1    1     2     2     2     2\n",
              "1    1    1    1    1     2     2     2     2\n",
              "2    1    1    1    1     2     2     2     2\n",
              "3    1    1    1    1     1     2     2     2\n",
              "4    1    0    1    1     2     0     2     2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86IGTsM3eIgP",
        "outputId": "5836b89b-fa7a-4d7b-de00-a04e4c71a149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Augment labels to match spectrograms\n",
        "aug_labs = []\n",
        "data_Y = labels_tdf['val'].to_numpy()[0:920]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_XiFrfooaMZ"
      },
      "source": [
        "\n",
        "\n",
        "# Train the CNN\n",
        "def train_net(train_set, no_epochs, lr, m, opt, classes, signal='Single'):\n",
        "\n",
        "    #Define the loss and optimiser\n",
        "    \n",
        "    #loss_func = nn.CrossEntropyLoss()\n",
        "    loss_func = nn.BCELoss()\n",
        "    #loss_func = nn.NLLLoss()\n",
        "    #loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    if opt == 'ADAM':\n",
        "      optimizer = optim.Adam(net.parameters(), lr = lr)\n",
        "    elif opt == 'SGD':\n",
        "      optimizer=optim.SGD(net.parameters(), lr = lr, momentum = m)\n",
        "\n",
        "    best_val_loss = 10000\n",
        "    best_epoch = 0\n",
        "    best_val_acc = 0\n",
        "\n",
        "    losses_train = []\n",
        "    losses_val = []\n",
        "\n",
        "    # Loop over the number of epochs\n",
        "    for epoch in range(no_epochs):\n",
        "\n",
        "        #Initialise loss and acc\n",
        "        current_loss = 0.0\n",
        "        current_accuracy = 0.0\n",
        "\n",
        "        # Loop over each mini-batch\n",
        "        for batch_index, training_batch in enumerate(train_set, 0):\n",
        "  \n",
        "            #Load the mini-batch and wrap with variable\n",
        "            if signal == 'Single':\n",
        "              inputs, labels = training_batch\n",
        "              inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "            if signal == 'Multi':\n",
        "              inputs, inputs2, labels = training_batch\n",
        "              inputs, inputs2, labels = Variable(inputs), Variable(inputs2), Variable(labels)\n",
        "\n",
        "            #Initalise parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #Forward pass\n",
        "            if signal == 'Single':\n",
        "              outputs = net.forward(inputs)\n",
        "            \n",
        "            if signal == 'Multi':\n",
        "              outputs = net.forward(inputs, inputs2)\n",
        "\n",
        "            #labels = labels.long()\n",
        "            loss = loss_func(outputs, labels)\n",
        "            \n",
        "            #Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #Add loss \n",
        "            current_loss += loss.item()\n",
        "\n",
        "            #Add accuracy to the overall accuracy\n",
        "            #current_accuracy += accuracy_2(training_batch) \n",
        "            if signal == 'Single':\n",
        "              current_accuracy += accuracy(inputs,'_',labels, signal=signal)\n",
        "            if signal == 'Multi':\n",
        "              current_accuracy += accuracy(inputs,inputs2,labels,signal=signal)\n",
        "\n",
        "        print('[Epoch: %d Batch: %5d] loss: %.3f, acc: %.3f' % (epoch + 1, batch_index+1, current_loss/batchsize, current_accuracy/batchsize))\n",
        "        losses_train.append(current_loss/batchsize)\n",
        "\n",
        "    print(\"------------------------------------------------------\")\n",
        "    print('Training has finished')\n",
        "    print('Best Epoch :',best_epoch,'Best Validation Loss :', best_val_loss)\n",
        "\n",
        "    if signal == 'Single':\n",
        "      outputs = net.forward(testf_tensor)\n",
        "      #outputs = F.softmax(outputs, dim = 0)\n",
        "    if signal == 'Multi':\n",
        "      outputs = net.forward(testf_tensor1, testf_tensor2)\n",
        "    \n",
        "    #Predict test accuracy\n",
        "    #_, preds = torch.max(outputs.data,1)\n",
        "    preds = []\n",
        "    for el in outputs.data:\n",
        "      if el >= 0.5:\n",
        "        preds.append(1)\n",
        "      elif el < 0.5:\n",
        "        preds.append(0)\n",
        "\n",
        "    num_correct = (torch.tensor(np.asarray(preds)).int() == testl_tensor.int()).sum().numpy()\n",
        "    test_ac = num_correct/(testl_tensor.size()[0])\n",
        "    print('Test Accuracy: ',test_ac*100)\n",
        "\n",
        "    #Test F1 score\n",
        "    true_y = testl_tensor.detach().numpy()\n",
        "    F1_test = sklearn.metrics.f1_score(true_y, preds, average = 'weighted')\n",
        "    print('Test F1 Score :', F1_test)\n",
        "    print(\"All :\",sklearn.metrics.classification_report(true_y, preds))\n",
        "    print(\"Confusion Matrix :\")\n",
        "    print(sklearn.metrics.confusion_matrix(true_y, preds))\n",
        "    #sklearn.metrics.plot_confusion_matrix(best_model, testf_tensor)\n",
        "    \n",
        "    #Plot learning curves\n",
        "    fig = plt.figure(figsize=plt.figaspect(0.2))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.plot(losses_train,'r')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "    return test_ac*100, F1_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqCxe-MD1Lfg",
        "outputId": "9fa59840-fb07-43c6-fbe1-b7421eaaf423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#Split augmented spectrogams and labels into 23 sets - 1 for each participant \n",
        "sub_splits_ppg = np.split(ppg_full_array, 23)\n",
        "sub_splits_resp = np.split(resp_full_array, 23)\n",
        "sub_splits_labs = np.split(data_Y, 23)\n",
        "print(np.asarray(sub_splits_resp).shape)\n",
        "print(np.asarray(sub_splits_labs).shape)\n",
        "subjects = [x for x in range(0,23,1)]\n",
        "print(subjects)\n",
        "image_size = np.asarray(sub_splits_ppg).shape[2]\n",
        "print(image_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23, 40, 28, 28)\n",
            "(23, 40)\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWl2iVYm6cEG"
      },
      "source": [
        "#Set a number of epochs\n",
        "no_epochs = 300\n",
        "\n",
        "# Set the learning rate\n",
        "lr = 0.5\n",
        "\n",
        "# Set the momentum\n",
        "momentum = 0.9\n",
        "\n",
        "accuracies_test = []\n",
        "f1s = []\n",
        "\n",
        "#Choose signal to train on \n",
        "dat = sub_splits_resp\n",
        "\n",
        "#Train model 23 times - differnet particiapnt data each time \n",
        "for s in subjects:\n",
        "  #Get training folds\n",
        "  train_inds = np.where(np.asarray(subjects) != s )[0]\n",
        "  train_feats = np.take(np.asarray(dat), train_inds, axis = 0)\n",
        "  train_feats = np.vstack(train_feats)\n",
        "  train_labs = np.take(np.asarray(sub_splits_labs), train_inds, axis = 0)\n",
        "  train_labs = np.reshape(train_labs, (train_feats.shape[0]))\n",
        "\n",
        "  #Get test fold\n",
        "  test_feats = np.take(np.asarray(dat), s, axis = 0)\n",
        "  test_labs = np.take(np.asarray(sub_splits_labs), s, axis = 0)\n",
        "  test_labs = np.reshape(test_labs, (test_feats.shape[0]))\n",
        "\n",
        "  #Reshape train and test arrays to specify number of input chanenels\n",
        "  train_feats = train_feats.reshape(len(train_feats),1,image_size,image_size)\n",
        "  test_feats = test_feats.reshape(len(test_feats),1,image_size,image_size)\n",
        "\n",
        "  #Convert to torch tensors\n",
        "  trainf_tensor = torch.tensor(train_feats, dtype=torch.float)\n",
        "  trainl_tensor = torch.tensor(train_labs, dtype=torch.float)\n",
        "  testf_tensor = torch.tensor(test_feats, requires_grad=False, dtype=torch.float)\n",
        "  testl_tensor = torch.tensor(test_labs, requires_grad=False, dtype=torch.float)\n",
        "  print(trainf_tensor.shape)\n",
        "  print(trainl_tensor.shape)\n",
        "  print(testf_tensor.shape)\n",
        "  print(testl_tensor.shape)\n",
        "\n",
        "  #Batchify training data using trainloader\n",
        "  batch_size = 50\n",
        "  trainset = torch.utils.data.TensorDataset(trainf_tensor, trainl_tensor)\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "  batchsize = len(trainloader)\n",
        "  print(batchsize)\n",
        "\n",
        "  print(\"Subject : \",s+1)\n",
        "  net = Single_Net(288)\n",
        "  test_accuracy,testf1 = train_net(trainloader, no_epochs, lr, momentum, opt = 'SGD', classes = 'binary', signal = 'Single')   \n",
        "  accuracies_test.append(test_accuracy) \n",
        "  f1s.append(testf1)\n",
        "\n",
        "#Print average metrics across all participants\n",
        "avg_acc = sum(accuracies_test)/len(accuracies_test)\n",
        "print(\"Accuracies :\", accuracies_test)\n",
        "print(\"Average test accuracy after 10 runs : {}%\".format(round(avg_acc,2)))\n",
        "print(\"Average test F1 Score :\", sum(f1s)/len(f1s))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}